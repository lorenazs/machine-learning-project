---
title: "Machine Learning course project"
author: "Lorena Zuniga"
date: "December 25, 2015"
output: html_document
---

## Context

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. 
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 


## The goal

It is  to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and predict the manner in which they did the exercise.

## The data

The dataset was provided by this source: [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)

## The process

### 1. First load the required libraries 
 
```{r , echo=FALSE}
library(caret)
library(rpart)
library(rattle)
library(rpart.plot)
library(randomForest)
```

### 2. Load the data for training and the final 20 tests
```{r}
 trData<- read.csv("data/pml-training.csv",na.strings=c("NA","#DIV/0!",""))
 tstData <- read.csv("data/pml-testing.csv",na.strings=c("NA","#DIV/0!",""))
```

### 3. Transforming the data (both training and test)
 in this case, the columns that are not necessary or relevant for the analysis where removed
 these are the fist five columns: X, user_name,raw_timestamp_part1, raw_timestamp_part_2
 cvtd_timestamp and new_window

```{r}
 dropColumns<-c('X', 'user_name','raw_timestamp_part_1', 'raw_timestamp_part_2','cvtd_timestamp','new_window')
 trData<- trData[,!(names(trData) %in% dropColumns)] 
 tstData<- tstData[,!(names(tstData) %in% dropColumns)]
```
 
In order to work with the cleanest data the columns where the NAs percentaje is greater than 40% were removed from
the original training and test datasets

```{r} 
 trData<-trData[,colSums(is.na(trData)) < nrow(trData) * 0.4]
 dim(trData)
 tstData<-tstData[,colSums(is.na(tstData)) < nrow(tstData) * 0.4]
 dim(tstData)
```
 


### 4.Creating the training and test sets
```{r}
 set.seed(1234)
 inTrain <- createDataPartition(y=trData$classe, p=0.7,list=FALSE)

 training <- trData[inTrain,]
 testing <- trData[-inTrain,]
```
### 5. First model with a simple decision tree

```{r}
 model <- train(training$classe ~ . , method='rpart',data=training)
 fancyRpartPlot(model$finalModel)
 predictionsDT <- predict(model,testing)
``` 
 
 Viewing the confusion matrix and the accuracy of the model

```{r} 
 confusionMatrix(predictionsDT,testing$classe)
```


### A second model using RandomForests
 
 #model2 <- train(training$classe ~. , data=training,method='rf',prox=TRUE)
```{r} 
 model2<-randomForest(classe ~ . ,data=training)
 
 # predictions with randomForest 
 predictionsRF <- predict(model2,testing)
```

Showing the confusion matrix and the model's accuracy

```{r}
confusionMatrix(predictionsRF,testing$classe)
```

 
### 6. predicting the values for the 20 tests using the second model (random forest)
Given the accuracy of the first model (0.48) vs the second one (0.99), the 20 records were tested with the random Forest model
```{r} 
 finalPredictionsRF <- predict(model2,tstData,type='class')
```

### The code provided to generate the files for submission
```{r}
 pml_write_files <-function(x){
   n = length(x)
   for(i in 1:n)
   {
     filename = paste0("problem_id_",i,".txt")
     write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names = FALSE)
   }
    
 } 
```
### 7. Creating the output files
```{r}
 pml_write_files(finalPredictionsRF)
 
```